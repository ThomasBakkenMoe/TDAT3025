{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "# Set up environment\n",
    "import gym\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "# Initial observation\n",
    "observation = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for discretizising the env state\n",
    "# The data from the env is continious and needs to be discretized befored being passed to the agent\n",
    "def discretize_state(self, observation):\n",
    "        discretized = list()\n",
    "        for i in range(len(obs)):\n",
    "            scaling = ((obs[i] + abs(self.lower_bounds[i]))\n",
    "                      / (self.upper_bounds[i] - self.lower_bounds[i]))\n",
    "            new_obs = int(round((self.buckets[i] - 1) * scaling))\n",
    "            new_obs = min(self.buckets[i] - 1, max(0, new_obs))\n",
    "            discretized.append(new_obs)\n",
    "        return tuple(discretized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent class\n",
    "class agent:\n",
    "    # Constructor\n",
    "    def __init__(self, buckets=(3, 3, 6, 6), num_episodes=500, min_learning_rate=0.1, min_epsilon=0.1, discount=1.0, decay=25):\n",
    "        \n",
    "        self.buckets = buckets\n",
    "        self.num_episodes = num_episodes\n",
    "        self.min_learning_rate = min_learning_rate\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.discount = discount\n",
    "        self.decay = decay\n",
    "        \n",
    "        \n",
    "        self.q_table = np.zeros(self.buckets + (env.action_space.n,))\n",
    "        \n",
    "        # Upper and lower limits\n",
    "        self.upper_limits = [env.observation_space.high[0], 0.5, env.observation_space.high[2], math.radians(50)/1.]\n",
    "        self.lower_limits = [env.observation_space.low[0], -0.5, env.observation_space.low[2], -math.radians(50)/1.]\n",
    "        \n",
    "        self.steps = np.zeros(self.num_episodes)\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        \n",
    "        if (np.random.random() < self.epsilon):\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space Discrete(2)\n",
      "State Space Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))\n",
    "\n",
    "for _ in range(1000):\n",
    "  env.render()\n",
    "  action = env.action_space.sample() # your agent here (this takes random actions)\n",
    "  observation, reward, done, info = env.step(action)\n",
    "\n",
    "  if done:\n",
    "    observation = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
