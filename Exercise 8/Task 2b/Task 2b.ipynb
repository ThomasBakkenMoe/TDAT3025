{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as functional\n",
    "\n",
    "# Set up environment\n",
    "import gym\n",
    "env = gym.make(\"CartPole-v0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "episodes = 500\n",
    "hidden_layer_size = 256\n",
    "learning_rate = 0.001  # Initial learning rate\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensors\n",
    "FloatTensor = torch.FloatTensor\n",
    "LongTensor = torch.LongTensor\n",
    "ByteTensor = torch.ByteTensor\n",
    "Tensor = FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class replay_buffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        \n",
    "    def push(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "        \n",
    "        # If the buffer gets filled above capacity, we delete the oldest entry [0]\n",
    "        if len(self.buffer) > self.capacity:\n",
    "            del self.buffer[0]\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(4, hidden_layer_size)\n",
    "        self.l2 = nn.Linear(hidden_layer_size, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = functional.relu(self.l1(x))\n",
    "        x = self.l2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent class\n",
    "class cart_pole_agent:\n",
    "    # Constructor\n",
    "    def __init__(self, min_learning_rate=0.1, min_epsilon=0.1, discount=0.8, decay_rate=25):\n",
    "        \n",
    "        self.min_learning_rate = min_learning_rate\n",
    "        self.learning_rate = min_learning_rate\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.epsilon = min_epsilon\n",
    "        self.discount = discount\n",
    "        self.decay_rate = decay_rate\n",
    "        \n",
    "        \n",
    "        #self.steps = np.zeros(self.num_episodes)\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        \n",
    "        if (np.random.random() < self.epsilon):\n",
    "            return nn_model(Variable(state).type(FloatTensor)).data.max(1)[1].view(1,1)\n",
    "        else:\n",
    "            return LongTensor([[random.randrange(2)]])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = neural_net()\n",
    "    \n",
    "buffer = replay_buffer(10000)\n",
    "optimizer = optim.Adam(nn_model.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\thomas bakken moe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:38: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has finished!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "agent = cart_pole_agent()\n",
    "\n",
    "# Train the agent\n",
    "for episode in range(episodes):\n",
    "    # Get initial state from the environment\n",
    "    current_state = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        action = agent.choose_action(FloatTensor([current_state]))\n",
    "        new_state, reward, done, _ = env.step(action[0, 0].item())\n",
    "        \n",
    "        if done:\n",
    "            reward = -1\n",
    "        \n",
    "        buffer.push((FloatTensor([current_state]), action, FloatTensor([new_state]), FloatTensor([reward])))\n",
    "        \n",
    "        if len(buffer) < batch_size:\n",
    "            continue\n",
    "        \n",
    "        transitions = buffer.sample(batch_size)\n",
    "        \n",
    "        batch_state, batch_action, batch_next_state, batch_reward = zip(*transitions)\n",
    "        \n",
    "        batch_state = Variable(torch.cat(batch_state))\n",
    "        batch_action = Variable(torch.cat(batch_action))\n",
    "        batch_reward = Variable(torch.cat(batch_reward))\n",
    "        batch_next_state = Variable(torch.cat(batch_next_state))\n",
    "        \n",
    "        current_q_values = nn_model(batch_state).gather(1, batch_action)\n",
    "        \n",
    "        max_next_q_values = nn_model(batch_next_state).detach().max(1)[0]\n",
    "        \n",
    "        expected_q_values = batch_reward + (agent.decay_rate * max_next_q_values)\n",
    "        \n",
    "        loss = functional.smooth_l1_loss(current_q_values, expected_q_values)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        current_state = new_state\n",
    "        \n",
    "\n",
    "print(\"Training has finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the trained agent\n",
    "\n",
    "episodes = 10\n",
    "for episode in range(episodes):\n",
    "    done = False\n",
    "    current_state = env.reset()\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = agent.choose_action(FloatTensor([current_state]))\n",
    "        new_state, reward, done, _ = env.step(action[0, 0].item())\n",
    "        \n",
    "        buffer.push((FloatTensor([current_state]), action, FloatTensor([new_state]), FloatTensor([reward])))\n",
    "        \n",
    "        current_state = new_state\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
