{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 193] <no description> Error loading \"c:\\users\\thomas bakken moe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\lib\\caffe2_detectron_ops_gpu.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-623823fa6eb4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thomas bakken moe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    114\u001b[0m                 \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWinError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrerror\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m' Error loading \"{}\" or one of its dependencies.'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdll\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m                 \u001b[0mis_loaded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 193] <no description> Error loading \"c:\\users\\thomas bakken moe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\lib\\caffe2_detectron_ops_gpu.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as functional\n",
    "\n",
    "# Set up environment\n",
    "import gym\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# Upper and lower limits\n",
    "upper_limits = [env.observation_space.high[0], 0.5, env.observation_space.high[2], math.radians(50)/1.]\n",
    "lower_limits = [env.observation_space.low[0], -0.5, env.observation_space.low[2], -math.radians(50)/1.]\n",
    "buckets=(3, 3, 6, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-9a9535d42db9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Cuda check\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0muse_cuda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mFloatTensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mLongTensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mByteTensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mByteTensor\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Cuda check\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for discretizising the env state\n",
    "# The data from the env is continious and needs to be discretized befored being passed to the agent\n",
    "def discretize_state(state):\n",
    "        discretized = list()\n",
    "        for i in range(len(state)):\n",
    "            scaling = ((state[i] + abs(lower_limits[i]))\n",
    "                      / (upper_limits[i] - lower_limits[i]))\n",
    "            new_state = int(round((buckets[i] - 1) * scaling))\n",
    "            new_state = min(buckets[i] - 1, max(0, new_state))\n",
    "            discretized.append(new_state)\n",
    "        return tuple(discretized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent class\n",
    "class cart_pole_agent:\n",
    "    # Constructor\n",
    "    def __init__(self, min_learning_rate=0.1, min_epsilon=0.1, discount=1.0, decay_rate=25):\n",
    "        \n",
    "        self.min_learning_rate = min_learning_rate\n",
    "        self.learning_rate = min_learning_rate\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.epsilon = min_epsilon\n",
    "        self.discount = discount\n",
    "        self.decay_rate = decay_rate\n",
    "        \n",
    "        \n",
    "        self.q_table = np.zeros(buckets + (env.action_space.n,))\n",
    "        \n",
    "        #self.steps = np.zeros(self.num_episodes)\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        \n",
    "        if (np.random.random() < self.epsilon):\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])\n",
    "        \n",
    "    def get_learning_rate(self, episode_number):\n",
    "        \n",
    "        # Returns the current learning rate of the agent.\n",
    "        # The learning rate decreases as the episode_number increases.\n",
    "        # The learning rate cannot go below the min_learning_rate.\n",
    "        return max(self.min_learning_rate, min(1.0, 1.0 - math.log10((episode_number+1)/self.decay_rate)))\n",
    "    \n",
    "    def get_epsilon(self, episode_number):\n",
    "        \n",
    "        # Returns the current epsilon value.\n",
    "        # The epsilon is responsible for random exploration.\n",
    "        # As we \"approach\" a splution, we want the amount of randomness to decrease\n",
    "        return max(self.min_epsilon, min(1.0, 1.0 - math.log10((episode_number+1)/ self.decay_rate)))\n",
    "    \n",
    "    def update_q_table(self, state, action, reward, new_state):\n",
    "        \n",
    "        self.q_table[state][action] += (self.learning_rate * (reward + self.discount * np.max(self.q_table[new_state]) - self.q_table[state][action]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has finished!\n"
     ]
    }
   ],
   "source": [
    "episodes = 500\n",
    "agent = cart_pole_agent()\n",
    "\n",
    "# Train the agent\n",
    "for episode in range(episodes):\n",
    "    # Get initial state from the environment\n",
    "    current_state = discretize_state(env.reset())\n",
    "    \n",
    "    agent.learning_rate = agent.get_learning_rate(episode)\n",
    "    agent.epsilon = agent.get_epsilon(episode)\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        action = agent.choose_action(current_state)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        new_state = discretize_state(new_state)\n",
    "        \n",
    "        agent.update_q_table(current_state, action, reward, new_state)\n",
    "        \n",
    "        current_state = new_state\n",
    "        \n",
    "\n",
    "print(\"Training has finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the trained agent\n",
    "\n",
    "episodes = 5\n",
    "for episode in range(episodes):\n",
    "    done = False\n",
    "    current_state = discretize_state(env.reset())\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = agent.choose_action(current_state)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        new_state = discretize_state(new_state)\n",
    "        current_state = new_state\n",
    "    \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
