{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "# Set up environment\n",
    "import gym\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# Upper and lower limits\n",
    "upper_limits = [env.observation_space.high[0], 0.5, env.observation_space.high[2], math.radians(50)/1.]\n",
    "lower_limits = [env.observation_space.low[0], -0.5, env.observation_space.low[2], -math.radians(50)/1.]\n",
    "buckets=(3, 3, 6, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for discretizising the env state\n",
    "# The data from the env is continious and needs to be discretized befored being passed to the agent\n",
    "def discretize_state(state):\n",
    "        discretized = list()\n",
    "        for i in range(len(state)):\n",
    "            scaling = ((state[i] + abs(lower_limits[i]))\n",
    "                      / (upper_limits[i] - lower_limits[i]))\n",
    "            new_state = int(round((buckets[i] - 1) * scaling))\n",
    "            new_state = min(buckets[i] - 1, max(0, new_state))\n",
    "            discretized.append(new_state)\n",
    "        return tuple(discretized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent class\n",
    "class cart_pole_agent:\n",
    "    # Constructor\n",
    "    def __init__(self, min_learning_rate=0.1, min_epsilon=0.1, discount=1.0, decay_rate=25):\n",
    "        \n",
    "        self.min_learning_rate = min_learning_rate\n",
    "        self.learning_rate = min_learning_rate\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.epsilon = min_epsilon\n",
    "        self.discount = discount\n",
    "        self.decay_rate = decay_rate\n",
    "        \n",
    "        \n",
    "        self.q_table = np.zeros(buckets + (env.action_space.n,))\n",
    "        \n",
    "        #self.steps = np.zeros(self.num_episodes)\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        \n",
    "        if (np.random.random() < self.epsilon):\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])\n",
    "        \n",
    "    def get_learning_rate(self, episode_number):\n",
    "        \n",
    "        # Returns the current learning rate of the agent.\n",
    "        # The learning rate decreases as the episode_number increases.\n",
    "        # The learning rate cannot go below the min_learning_rate.\n",
    "        return max(self.min_learning_rate, min(1.0, 1.0 - math.log10((episode_number+1)/self.decay_rate)))\n",
    "    \n",
    "    def get_epsilon(self, episode_number):\n",
    "        \n",
    "        # Returns the current epsilon value.\n",
    "        # The epsilon is responsible for random exploration.\n",
    "        # As we \"approach\" a splution, we want the amount of randomness to decrease\n",
    "        return max(self.min_epsilon, min(1.0, 1.0 - math.log10((episode_number+1)/ self.decay_rate)))\n",
    "    \n",
    "    def update_q_table(self, state, action, reward, new_state):\n",
    "        \n",
    "        self.q_table[state][action] += (self.learning_rate * (reward + self.discount * np.max(self.q_table[new_state]) - self.q_table[state][action]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has finished!\n"
     ]
    }
   ],
   "source": [
    "episodes = 500\n",
    "agent = cart_pole_agent()\n",
    "\n",
    "# Train the agent\n",
    "for episode in range(episodes):\n",
    "    # Get initial state from the environment\n",
    "    current_state = discretize_state(env.reset())\n",
    "    \n",
    "    agent.learning_rate = agent.get_learning_rate(episode)\n",
    "    agent.epsilon = agent.get_epsilon(episode)\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        action = agent.choose_action(current_state)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        new_state = discretize_state(new_state)\n",
    "        \n",
    "        agent.update_q_table(current_state, action, reward, new_state)\n",
    "        \n",
    "        current_state = new_state\n",
    "        \n",
    "\n",
    "print(\"Training has finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the trained agent\n",
    "\n",
    "episodes = 5\n",
    "for episode in range(episodes):\n",
    "    done = False\n",
    "    current_state = discretize_state(env.reset())\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = agent.choose_action(current_state)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        new_state = discretize_state(new_state)\n",
    "        current_state = new_state\n",
    "    \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
